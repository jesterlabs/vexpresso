{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88f5cb6-2b03-418a-b726-faca6dbe8427",
   "metadata": {},
   "source": [
    "# Querying Audio with CLAP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21c7d4-859d-405f-a06e-8f00b898703e",
   "metadata": {},
   "source": [
    "## In this walkthrough, we will be using a dataset of audio files and embed them using the CLAP model (https://huggingface.co/docs/transformers/v4.30.0/en/model_doc/clap#transformers.ClapModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19c592-79a2-46c4-807f-0288742cb1fc",
   "metadata": {},
   "source": [
    "## Installation Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99533903-13bc-4103-95f4-f7becf02a27c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc2f21-388b-419f-a26f-2619c262e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, ClapModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import vexpresso\n",
    "from vexpresso.utils import ResourceRequest, DataType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b775fd6-a215-464b-a22e-1963865e32b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f8cff-0a6f-47b6-8438-5bde8d8f6c8e",
   "metadata": {},
   "source": [
    "Here we load a dataset of audio files from https://huggingface.co/datasets/ashraq/esc50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44af8a-8a2b-45a3-9e6f-9aabc16833a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ashraq/esc50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6344986-2a05-4ad6-a725-39d126a54465",
   "metadata": {},
   "source": [
    "Convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d35f8-294b-45cb-b060-a42f7fbfdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dataset['train'].to_dict()\n",
    "audios = dataset['train']['audio']\n",
    "dictionary['audio'] = audios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69bb4f-441e-4194-badc-c0911541b5b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4e094-c89b-4639-9b37-afbba213fbf0",
   "metadata": {},
   "source": [
    "Lets create a collection with the audios that we downloaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcaf55-588b-4939-bf71-26f52f409574",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = vexpresso.create(data=dictionary, backend=\"ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311da8f6-41e9-4a18-a3c6-770a0724ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556bf3e-668f-4dd3-9c04-0de75c5ad656",
   "metadata": {},
   "source": [
    "Let's filter out the B takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095bece-4d17-40da-b804-8230d0d16c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = collection.filter({\"take\":{\"eq\":\"A\"}}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb33df4-aa0f-47e1-885d-d018fb72869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb25ea-491d-4827-8e51-82834588c994",
   "metadata": {},
   "source": [
    "Lets take a look at the different categories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cedca8-8274-4f62-9bde-e0e05c538b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(collection[\"category\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cccdf9-cc7d-41ad-8a34-20440ca924aa",
   "metadata": {},
   "source": [
    "Because this is a demo, let's only get one sound from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d583e79-57a4-4235-99f9-3760693494db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_filter(category):\n",
    "    unique_set = set([])\n",
    "    out = []\n",
    "    for c in category:\n",
    "        if c not in unique_set:\n",
    "            out.append(\"valid\")\n",
    "            unique_set.add(c)\n",
    "        else:\n",
    "            out.append(None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bb4d2-add2-49cd-b6aa-381d6f29d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = collection.apply(unique_filter, collection[\"category\"], to=\"filter_valid\").filter({\"filter_valid\":{\"eq\":\"valid\"}}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abe269-86f3-4283-b45c-6887d07c93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540216d-edef-42f5-89b8-703271c500f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multimodal CLAP Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902202bf-1650-4c0e-b0ce-5862cf24e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClAPEmbeddingsFunction:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "        self.device = torch.device('cpu')\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "    def __call__(self, inp, inp_type):\n",
    "        if inp_type == \"audio\":\n",
    "            inputs = self.processor(audios=inp, return_tensors=\"pt\", padding=True)\n",
    "            print(inputs.keys())\n",
    "            for k in inputs:\n",
    "                inputs[k] = inputs[k].to(self.device)\n",
    "            return self.model.get_audio_features(**inputs).detach().cpu().numpy()\n",
    "        if inp_type == \"text\":\n",
    "            inputs = self.tokenizer(inp, padding=True, return_tensors=\"pt\")\n",
    "            inputs[\"input_ids\"] = inputs[\"input_ids\"].to(self.device)\n",
    "            inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(self.device)\n",
    "            return self.model.get_text_features(**inputs).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0b839-3b50-4c81-a8e7-349a29d55201",
   "metadata": {},
   "source": [
    "## Now lets embed the audio arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9410a-bf46-4d45-8422-fbb7d389863c",
   "metadata": {},
   "source": [
    "This may take a while because we're embedding 2000 audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dd029-7b13-4d91-8fed-58b548e9367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = collection.embed(collection[\"audio.array\"], inp_type=\"audio\", embedding_fn=ClAPEmbeddingsFunction, to=\"audio_embeddings\", resource_request=ResourceRequest(num_gpus=1)).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cd6c4-4a8c-4178-931f-52f3b074c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39] *",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
